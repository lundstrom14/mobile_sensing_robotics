{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Model - Theory notes\n",
    "\n",
    "*  Basic laws of physics to describe wheeled motion, (location, angular orientation, velocity)\n",
    "$$\n",
    "\\begin{align}\n",
    "s = & \\:vt \\\\\n",
    "\\theta = & \\:\\omega t\\\\ \n",
    "v = & \\:\\omega r\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "## Probalistic Motion Models\n",
    "\n",
    "- Motion is inherently uncertain\n",
    "- How can we model this uncertainty?\n",
    "$$\n",
    "p(x_t \\:|u_t, \\: x_{t-1})\n",
    "$$\n",
    "\n",
    "Which occurs in the Bayes Filter\n",
    "\n",
    "* **Prediction** \n",
    "$$\n",
    "bel(x_t) = \\int p (x_t\\:|\\:u_t,x_{t-1})bel(x_{t-1})dx_{t-1}\n",
    "$$\n",
    "* **Correction**\n",
    "$$\n",
    "bel(x_t) = \\eta\\: p( z_t\\:|x_t)\\: bel(x_t)\n",
    "$$\n",
    "\n",
    "The motion model specifies a posterior probability that action u carries the robot from $x_{t-1}$ to $x_t$\n",
    "\n",
    "***\n",
    "\n",
    "### Typical Motion models\n",
    "\n",
    "Two common types of motion models:\n",
    "* **Odometry based** (wheel encoders)\n",
    "* **Velocity based** (dead reconning)\n",
    "\n",
    "**Odometry Model**\n",
    "* Motion from $[\\bar{x},\\bar{y},\\bar{\\theta}]$ to $[\\bar{x}',\\bar{y}',\\bar{\\theta}']$\n",
    "* Odometry information $ u = (\\delta_{rot1}, \\delta_{trans}, \\delta_{rot2}) $\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_{trans} = & \\sqrt{(\\bar{x}'-\\bar{x})^2 + (\\bar{y}'-\\bar{y})^2} \\\\\n",
    "\\delta_{rot1} = & \\text{atan2}((\\bar{y}'-\\bar{y},\\bar{x}'-\\bar{x}) - \\bar{\\theta} \\\\\n",
    "\\delta_{rot2} = & \\bar{\\theta}' - \\bar{\\theta} - \\delta_{rot1}\n",
    "\\end{align}\n",
    "$$\n",
    "<img src=\"./res/odometry_model.png\" alt=\"laser_scan\" width=\"400\"/>\n",
    "\n",
    "We could assume Gaussian noise $u \\sim  \\mathcal{N}(0,\\sum)$ \n",
    "\n",
    "**Noise Model**\n",
    "\n",
    "The measured motion is given by the true motion corrupted with noise. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\delta}_{trans} = & \\:\\delta_{trans} + \\epsilon_{..}\\\\\n",
    "\\hat{\\delta}_{rot1} = & \\:\\delta_{rot1} + \\epsilon_{..}\\\\\n",
    "\\hat{\\delta}_{rot2} = & \\:\\delta_{rot2} + \\epsilon_{..}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "**_DISCLAIMER:_** The notation used in this exercise follows the one of the Probabilistic Robotics book (refer to Chapter 5.4 in case you have doubts).\n",
    "\n",
    "## 4.1 Inverse motion model\n",
    "\n",
    "The odometry model uses the _relative motion information_. The odometry readings are $u_t = [{\\overline{x}}_{t-1} , {\\overline{x}}_{t}]$, where $\\overline{x}_{t-1}$ and  $\\overline{x}_t$ are poses in a robot-internal coordinate frame (different from the map).\n",
    "\n",
    "The function `inverse_motion_model` takes as input an odometry reading $u_t$ that consist in:\n",
    "\n",
    "- the initial pose of the robot in the odometry coordinate frame $\\overline{x}_{t-1} = [\\overline{x},\\overline{y},\\overline{\\theta}]$\n",
    "- the estimated pose of the robot in the odometry coordinate frame $\\overline{x}_t = [\\overline{x}',\\overline{y}',\\overline{\\theta}']$\n",
    "\n",
    "The output is the relative motion $\\delta_{rot1}, \\delta_{trans}, \\delta_{rot2}$.\n",
    "\n",
    "Implement the function `inverse_motion_model` and verify that it is correct for some test input. **[2.0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4142135623730951, -1.4853981633974482, 1.4513981633974482)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Odometry-based motion model\n",
    "\n",
    "The function `motion_model_odometry` computes the posterior $p(x_t | u_t, x_{t-1})$ from odometry readings.\n",
    "\n",
    "This function takes as input:\n",
    "\n",
    "- the initial pose of the robot $x_{t-1} = [x,y,\\theta]$ _(**map** coordinate frame)_\n",
    "- the hypothesized (or query) final pose $x_{t} = [x', y', \\theta']$ _(**map** coordinate frame)_\n",
    "- the odometry readings $u_t = [\\overline{x}_{t-1} \\overline{x}_t]$ _(**odometry** coordinate frame)_\n",
    "- the noise parameters $\\mathbf{\\alpha} = [\\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4]$\n",
    "\n",
    "The output is the probability $p(x_t | u_t, x_{t-1})$\n",
    "\n",
    "Assume that a robot starts at pose $x_0 = [2.0, 3.0, 0.0]$ in the map frame and moves according to a motion model with $\\mathbf{\\alpha} = [1.0, 1.0, 0.01, 0.01]$.\n",
    "\n",
    "The robot excecutes one motion command and the odometry readings are:\n",
    "\n",
    "1. $\\overline{x}_0 = [0.0 , 0.0 , 0.0   ]$\n",
    "2. $\\overline{x}_1 = [0.5 , 0.0 , \\pi/2 ]$\n",
    "\n",
    "Implement the `motion_model_odometry` function and verify that it is correct for some test input. **[1.0]**\n",
    "\n",
    "---\n",
    "\n",
    "Consider a 150x150 grid map the world with a resolution of 0.01, centered in the original position of the robot.\n",
    "\n",
    "Plot the posterior $p(x_t | u_t, x_{t-1})$ for all possible $[x, y]$ values from the grid. **[2.0]**\n",
    "\n",
    "**Note that** the query input is a position, not a pose. Therefore, to plot the posterior belief over the gridmap, you can assume the term $\\hat{\\delta}_\\mathrm{rot2}$ to be zero and, for each position, integrate over all possible orientations. This can be implemented by considering $p_3 = 1.0$ in the equations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Sample odometry motion model\n",
    "\n",
    "The `motion_model_odometry` requires high computation complexity and does not scale well to large real-world environments. \n",
    "\n",
    "One effective approach to approximate $p(x_t | u_t, x_{t-1})$ is to use **sampling**.\n",
    "\n",
    "The `sample_motion_model_odometry` function defines the sampling-based odometry motion model. \n",
    "\n",
    "This function takes as input:\n",
    "\n",
    "- the initial pose of the robot $x_{t-1} = [x,y,\\theta]$ _(**map** coordinate frame)_\n",
    "- the odometry readings $u_t = [\\overline{x}_{t-1} \\overline{x}_t]$ _(**odometry** coordinate frame)_\n",
    "- the noise parameters $\\mathbf{\\alpha} = [\\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4]$\n",
    "\n",
    "The output is a new (sampled) pose predicted by the motion model.\n",
    "\n",
    "Implement the `sample_motion_model_odometry` function and verify that it is correct for some test input. **[2.0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluate sample odometry motion model\n",
    "\n",
    "Assume that a robot starts at pose $x_0 = [2.0, 3.0, 0.0]$ in the map frame and moves according to a motion model with $\\mathbf{\\alpha} = [0.1, 0.1, 0.01, 0.01]$.\n",
    "\n",
    "The robot obtains the following odometry readings:\n",
    "\n",
    "1. $\\overline{x}_0 = [0.0 , 0.0 , 0.0   ]$\n",
    "2. $\\overline{x}_1 = [0.5 , 0.0 , \\pi/2 ]$\n",
    "3. $\\overline{x}_2 = [0.5 , 0.5 , 0.0   ]$\n",
    "4. $\\overline{x}_3 = [1.0 , 0.5 , 0.0   ]$\n",
    "5. $\\overline{x}_4 = [1.0 , 1.5 , \\pi/2 ]$\n",
    "6. $\\overline{x}_5 = [1.0 , 2.5 , \\pi/2 ]$\n",
    "\n",
    "Evaluate the `sample_motion_model_odometry` by considering 1000 samples and plot the resulting positions for each sample in one unique plot. **[3.0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
